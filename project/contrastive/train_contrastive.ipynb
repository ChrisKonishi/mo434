{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-snippets"
      ],
      "metadata": {
        "id": "B2lv1jq_-m-U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9QGgsSDV-oxe",
        "outputId": "7f8769c9-cc70-42b1-f098-7b2b72ad15b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/Shareddrives/MO434/dataset/project_dataset_corel_ct.zip .\n",
        "!unzip -q project_dataset_corel_ct.zip"
      ],
      "metadata": {
        "id": "eHwJD8ms_Bla"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZtjiatO19o66",
        "outputId": "38a9fe61-cd4d-4118-b09d-795bee0d6340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[07/05/23 21:35:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;31mWARNING \u001b[0m Unable to load torch and dependent libraries from                \u001b]8;id=598121;file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py\u001b\\\u001b[2mloader.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=984820;file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py#<module>:98\u001b\\\u001b[2m<module>:98\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         torch-snippets.                                                  \u001b[2m                     \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         Functionalities might be limited. pip install lovely-tensors in  \u001b[2m                     \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         case there are torch related errors                              \u001b[2m                     \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/05/23 21:35:49] </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f\">WARNING </span> Unable to load torch and dependent libraries from                <a href=\"file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">loader.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py#<module>:98\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;module&gt;:98</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         torch-snippets.                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Functionalities might be limited. pip install lovely-tensors in  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         case there are torch related errors                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torch_snippets import *\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "\n",
        "batchsize = 32\n",
        "n_fold = 3\n",
        "input_shape = (3,224,224)\n",
        "device = torch.device('cuda')\n",
        "test_only = False\n",
        "\n",
        "lr = 1e-6\n",
        "weight_decay=1e-3\n",
        "n_epochs = 50\n",
        "\n",
        "scheduler_step=20\n",
        "scheduler_gamma = 0.5\n",
        "\n",
        "dataset_dir = \"/content/project_dataset_corel\"\n",
        "#dataset_dir = \"../dataset/project_dataset_corel\"\n",
        "\n",
        "log_dir='/content/drive/Shareddrives/MO434/executions/ct/ct_freeze'\n",
        "#log_dir='./log'\n",
        "\n",
        "def create_dir_if_necessary(dir):\n",
        "    try:\n",
        "        os.makedirs(dir)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "create_dir_if_necessary(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1YqbKOjw9o67"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import typing\n",
        "import os.path as osp\n",
        "from PIL import Image\n",
        "\n",
        "class CtDataset():\n",
        "    def __init__(self, fold_dir, mode: typing.Literal['test', 'val', 'train'], transforms=None) -> None:\n",
        "        self.fold_dir = fold_dir\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.img_dir = osp.join(fold_dir, mode)\n",
        "        self.pair_df = pd.read_csv(osp.join(fold_dir, f'ct_{self.mode}.csv'), header=None)\n",
        "\n",
        "        self.img_dict = {}\n",
        "        img_list = pd.concat((self.pair_df[0], self.pair_df[1]), ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        for idx, item in img_list.items():\n",
        "            img_path = osp.join(self.img_dir, item)\n",
        "            img = Image.open(img_path)\n",
        "            self.img_dict[item] = img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pair_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pair_df.loc[idx, :]\n",
        "        img1 = self.img_dict[pair[0]]\n",
        "        img2 = self.img_dict[pair[1]]\n",
        "        if self.transforms:\n",
        "            img1 = self.transforms(img1)\n",
        "            img2 = self.transforms(img2)\n",
        "        return img1, img2, np.array([pair[2]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tkSmGZRo9o67"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.05,0.10), scale=(0.9,1.1), shear=(-2,2),\n",
        "                            interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                            fill=0),\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                      max_size=None, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    #AddGaussianNoise(0, 0.1)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                      max_size=None, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UUgc4wU29o68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class CMKNet(nn.Module):\n",
        "    def __init__(self, dropout=0) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input: 3, 224, 224\n",
        "        self.col1 = nn.Sequential(\n",
        "            self._conv_seq(3, 32, 3), # 32, 112, 112\n",
        "            self._conv_seq(32, 64, 3), # 64, 56, 56\n",
        "            self._conv_seq(64, 128, 3), # 128, 28, 28\n",
        "        )\n",
        "\n",
        "        self.dense = nn.Sequential(\n",
        "            self._linear_seq(128*28*28, 1024, dropout),\n",
        "            self._linear_seq(1024, 256, dropout),\n",
        "            nn.Linear(256, 64)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "        self.cam = False\n",
        "        self.cam_grad = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.col1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _register_grad(self, grad):\n",
        "        self.cam_grad = grad\n",
        "\n",
        "    def get_conv_gradient(self):\n",
        "        return self.cam_grad\n",
        "\n",
        "    def get_conv_activation(self, x):\n",
        "        return self.col1(x)\n",
        "\n",
        "    def get_output_per_layer(self, x):\n",
        "        output_by_layer = OrderedDict()\n",
        "\n",
        "        output_by_layer['input'] = x.clone().detach().cpu().data.numpy()\n",
        "\n",
        "        x = self.col1(x)\n",
        "        output_by_layer[\"featextract-conv\"] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        for layer_name, layer in self.dense.named_children():\n",
        "            #do forward through the layer\n",
        "            x = layer.forward(x)\n",
        "            #save the output\n",
        "            output_by_layer[\"classifier-\"+layer_name] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        return output_by_layer\n",
        "\n",
        "\n",
        "    def _fuse(self, in_channels, out_channels, kernel_size):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def _linear_seq(self, in_features, out_features, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_features, out_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def _conv_seq(self, in_channels, out_channels, kernel_size):\n",
        "        \"conv2d, batchnorm, relu, max pool\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        #for each submodule of our network\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                #get the number of elements in the layer weights\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = sqrt(2. / n))\n",
        "                m.weight.data.normal_(mean=0, std=math.sqrt(2. / n))\n",
        "\n",
        "                if m.bias is not None:\n",
        "                    #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = 1/100\n",
        "                m.weight.data.normal_(mean=0, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "def get_model_ct(params_dir=None):\n",
        "    model = CMKNet()\n",
        "    if params_dir:\n",
        "        model.load_state_dict(torch.load(params_dir, map_location=device))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aM0DOjsJ9o68"
      },
      "outputs": [],
      "source": [
        "contrastive_thres = 1.1\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2)/2 +\n",
        "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))/2\n",
        "        #print(list(zip(euclidean_distance.cpu().detach().numpy(), label.cpu().detach().numpy())))\n",
        "        #print(loss_contrastive)\n",
        "        acc = ((euclidean_distance > contrastive_thres) == label).float().mean()\n",
        "        return loss_contrastive, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6iZkdTGG9o68"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def dist_hist(best_model_path, trn_dl):\n",
        "    # Distance histograms for the cases we have the same and different persons, respectively.\n",
        "    model = get_model_ct(best_model_path).to(device)\n",
        "    different = []\n",
        "    same      = []\n",
        "    model.eval()\n",
        "\n",
        "    for ix, data in enumerate(trn_dl):\n",
        "        if (ix + 1)%100 == 0:\n",
        "            print('Processing batch {}/{}'.format((ix + 1), len(trn_dl)))\n",
        "        imgsA, imgsB, labels = [t.to(device) for t in data]\n",
        "        codesA, codesB       = model(imgsA), model(imgsB)\n",
        "        labels               = labels.cpu()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            euclidean_distance        = F.pairwise_distance(codesA, codesB)\n",
        "            euclidean_distance        = euclidean_distance.cpu()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            if (labels[i] == 0): # same person\n",
        "                same.append(euclidean_distance[i].item())\n",
        "            else:\n",
        "                different.append(euclidean_distance[i].item())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (10,5))\n",
        "    ax.set_title('Distance histograms for genuine and impostor comparisons', fontsize = 20, fontweight = 'bold')\n",
        "    ax.set_xlabel('Euclidean Distance', fontsize = 16, fontweight = 'bold')\n",
        "    ax.set_ylabel('Density', fontsize = 16, fontweight = 'bold')\n",
        "    ax.hist(different,bins = 50, density=True, alpha = 0.7, label = 'different')\n",
        "    ax.hist(same, bins = 50, density=True, alpha = 0.7, label = 'same')\n",
        "    ax.tick_params(labelsize = 16, axis = 'both')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    plt.plot()\n",
        "\n",
        "def train_batch_ct(model, data, optimizer, criterion):\n",
        "    imgsA, imgsB, labels = [t.to(device) for t in data]\n",
        "    optimizer.zero_grad()\n",
        "    codesA, codesB = model(imgsA), model(imgsB)\n",
        "    loss, acc = criterion(codesA, codesB, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item(), acc.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_ct(model, dataloder, criterion):\n",
        "    model.eval()\n",
        "    ac_acc, ac_loss = 0, 0\n",
        "    cnt = 0\n",
        "    for idx, data in enumerate(dataloder):\n",
        "        imgsA, imgsB, labels = [t.to(device) for t in data]\n",
        "        codesA, codesB = model(imgsA), model(imgsB)\n",
        "        loss, acc = criterion(codesA, codesB, labels)\n",
        "        ac_loss += loss\n",
        "        ac_acc += acc\n",
        "        cnt +=1\n",
        "    model.train()\n",
        "    return ac_loss.item()/cnt, ac_acc.item()/cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qm1vLztA9o69"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_ct(dataset_dir, best_model_path):\n",
        "    # DataLoaders\n",
        "    train_set = CtDataset(dataset_dir, 'train', transforms=train_transforms)\n",
        "    val_set = CtDataset(dataset_dir, 'val', transforms=val_transforms)\n",
        "\n",
        "    trn_dl = DataLoader(train_set, batch_size=batchsize, shuffle=True)\n",
        "    val_dl = DataLoader(val_set, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = get_model_ct().to(device)\n",
        "    model.train()\n",
        "\n",
        "    criterion = ContrastiveLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr = lr, weight_decay=weight_decay)\n",
        "\n",
        "    log = Report(n_epochs)\n",
        "\n",
        "    best_model = None\n",
        "    best_model_loss = sys.maxsize\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(trn_dl)\n",
        "        epoch_train_loss = 0\n",
        "        for i, data in enumerate(trn_dl):\n",
        "            loss, acc = train_batch_ct(model, data, optimizer, criterion)\n",
        "            epoch_train_loss += loss\n",
        "\n",
        "        loss, acc = validate_ct(model, val_dl, criterion)\n",
        "        if loss < best_model_loss:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_model_loss = loss\n",
        "            best_model = epoch\n",
        "\n",
        "        log.record(epoch+1, train_loss=epoch_train_loss/N, val_loss=loss, val_acc=acc,\n",
        "        best_model=best_model+1, best_loss=best_model_loss, end='\\r')\n",
        "\n",
        "    log.plot_epochs(['train_loss','val_loss'])\n",
        "    log.plot_epochs(['val_acc'])\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    dist_hist(best_model_path, trn_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0_yYD8O9o69",
        "outputId": "d921c135-af06-4d44-9efa-68eec1e74b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4.000  train_loss: 0.251  val_loss: 0.274  val_acc: 0.711  best_model: 4.000  best_loss: 0.274  (72.00s - 827.98s remaining)"
          ]
        }
      ],
      "source": [
        "for fold in range(n_fold):\n",
        "    fold_log_dir = osp.join(log_dir, f'fold{fold}')\n",
        "    create_dir_if_necessary(fold_log_dir)\n",
        "\n",
        "    train_ct(osp.join(dataset_dir, f'fold{fold}'), osp.join(fold_log_dir, 'best_model_ct.pth'))\n",
        "\n",
        "\n",
        "# necessario remover dropout"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}