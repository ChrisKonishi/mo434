{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S7GgBpmHcGat",
        "outputId": "e782a410-2136-4fe7-e242-73ef0b2183df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-snippets in /usr/local/lib/python3.10/dist-packages (0.499.34)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.5.29)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (8.4.0)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (4.2.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.3.6)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (7.34.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (4.65.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (13.4.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (6.0)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (2.0.8)\n",
            "Requirement already satisfied: confection in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.0.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.10.9)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.7.4.3)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (4.6.3)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.1.2)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.1.0)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.4.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.13.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.8.1)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.21.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.3.3)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.22.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (6.5.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (5.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (4.7.0.72)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (2.25.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (2.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torch-snippets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torch-snippets) (2022.7.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->torch-snippets) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->torch-snippets) (23.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (4.8.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->torch-snippets) (23.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (3.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->torch-snippets) (2.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->torch-snippets) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->torch-snippets) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->torch-snippets) (2022.10.31)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->torch-snippets) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->torch-snippets) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->torch-snippets) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->torch-snippets) (20.23.1)\n",
            "Requirement already satisfied: Levenshtein==0.21.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein->torch-snippets) (0.21.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.21.1->python-Levenshtein->torch-snippets) (3.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->torch-snippets) (3.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-snippets) (3.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->torch-snippets) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->torch-snippets) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->torch-snippets) (3.7.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->torch-snippets) (0.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->torch-snippets) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->torch-snippets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->torch-snippets) (0.2.6)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (1.4.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->torch-snippets) (0.3.6)\n",
            "Requirement already satisfied: filelock<4,>=3.12 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->torch-snippets) (3.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->torch-snippets) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->torch-snippets) (0.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->torch-snippets) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->torch-snippets) (6.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BHAwre4HcGau",
        "outputId": "1f22a0f6-e4b9-4236-9a0a-46c58e2b2048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/Shareddrives/MO434/dataset/project_dataset_corel_ct.zip .\n",
        "!unzip -q -n project_dataset_corel_ct.zip"
      ],
      "metadata": {
        "id": "N6c5fo1gcK6k",
        "outputId": "983fb466-0405-414b-a8a6-507e7f8e7f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace project_dataset_corel/fold0/.~lock.ct_val.csv#? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-5 (attachment_entry):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 237, in listen\n",
            "    sock, _ = endpoints_listener.accept()\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 293, in accept\n",
            "    fd, addr = self._accept()\n",
            "TimeoutError: timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/_debugpy.py\", line 52, in attachment_entry\n",
            "    debugpy.listen(_dap_port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/public_api.py\", line 31, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 143, in debug\n",
            "    log.reraise_exception(\"{0}() failed:\", func.__name__, level=\"info\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 141, in debug\n",
            "    return func(address, settrace_kwargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 251, in listen\n",
            "    raise RuntimeError(\"timed out waiting for adapter to connect\")\n",
            "RuntimeError: timed out waiting for adapter to connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEc3JAmlcGau"
      },
      "source": [
        "## Contrastive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "neFjxgRgcGav",
        "outputId": "24fd1d81-f21f-496a-f112-e275704a1265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[07/05/23 23:59:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;31mWARNING \u001b[0m Unable to load torch and dependent libraries from                \u001b]8;id=906588;file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py\u001b\\\u001b[2mloader.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=817743;file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py#<module>:98\u001b\\\u001b[2m<module>:98\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         torch-snippets.                                                  \u001b[2m                     \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         Functionalities might be limited. pip install lovely-tensors in  \u001b[2m                     \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         case there are torch related errors                              \u001b[2m                     \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/05/23 23:59:42] </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f\">WARNING </span> Unable to load torch and dependent libraries from                <a href=\"file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">loader.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/torch_snippets/loader.py#<module>:98\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;module&gt;:98</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         torch-snippets.                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Functionalities might be limited. pip install lovely-tensors in  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         case there are torch related errors                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torch_snippets import *\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "\n",
        "batchsize = 32\n",
        "n_fold = 3\n",
        "input_shape = (3,224,224)\n",
        "device = torch.device('cuda')\n",
        "test_only = False\n",
        "\n",
        "lr = 2e-5\n",
        "weight_decay=1e-3\n",
        "n_epochs = 60\n",
        "\n",
        "scheduler_step=20\n",
        "scheduler_gamma = 0.5\n",
        "\n",
        "dataset_dir = \"/content/project_dataset_corel\"\n",
        "#dataset_dir = \"../dataset/project_dataset_corel\"\n",
        "\n",
        "log_dir='/content/drive/Shareddrives/MO434/executions/ct/ct_freeze'\n",
        "#log_dir='./log'\n",
        "\n",
        "def create_dir_if_necessary(dir):\n",
        "    try:\n",
        "        os.makedirs(dir)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "create_dir_if_necessary(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vpcqZjxRcGav"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import typing\n",
        "import os.path as osp\n",
        "from PIL import Image\n",
        "\n",
        "class CtDataset():\n",
        "    def __init__(self, fold_dir, mode: typing.Literal['test', 'val', 'train'], transforms=None) -> None:\n",
        "        self.fold_dir = fold_dir\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.img_dir = osp.join(fold_dir, mode)\n",
        "        self.pair_df = pd.read_csv(osp.join(fold_dir, f'ct_{self.mode}.csv'), header=None)\n",
        "\n",
        "        self.img_dict = {}\n",
        "        img_list = pd.concat((self.pair_df[0], self.pair_df[1]), ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        for idx, item in img_list.items():\n",
        "            img_path = osp.join(self.img_dir, item)\n",
        "            img = Image.open(img_path)\n",
        "            self.img_dict[item] = img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pair_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pair_df.loc[idx, :]\n",
        "        img1 = self.img_dict[pair[0]]\n",
        "        img2 = self.img_dict[pair[1]]\n",
        "        if self.transforms:\n",
        "            img1 = self.transforms(img1)\n",
        "            img2 = self.transforms(img2)\n",
        "        return img1, img2, np.array([pair[2]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "91f7-f3ccGav"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.05,0.10), scale=(0.9,1.1), shear=(-2,2),\n",
        "                            interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                            fill=0),\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                      max_size=None, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    #AddGaussianNoise(0, 0.1)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                      max_size=None, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oQLyXRUfcGaw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class CMKNetCt(nn.Module):\n",
        "    def __init__(self, dropout=0) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input: 3, 224, 224\n",
        "        self.col1 = nn.Sequential(\n",
        "            self._conv_seq(3, 32, 3), # 32, 112, 112\n",
        "            self._conv_seq(32, 64, 3), # 64, 56, 56\n",
        "            self._conv_seq(64, 128, 3), # 128, 28, 28\n",
        "        )\n",
        "\n",
        "        self.dense = nn.Sequential(\n",
        "            self._linear_seq(128*28*28, 1024, dropout),\n",
        "            self._linear_seq(1024, 256, dropout),\n",
        "            nn.Linear(256, 64)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "        self.cam = False\n",
        "        self.cam_grad = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.col1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _register_grad(self, grad):\n",
        "        self.cam_grad = grad\n",
        "\n",
        "    def get_conv_gradient(self):\n",
        "        return self.cam_grad\n",
        "\n",
        "    def get_conv_activation(self, x):\n",
        "        return self.col1(x)\n",
        "\n",
        "    def get_output_per_layer(self, x):\n",
        "        output_by_layer = OrderedDict()\n",
        "\n",
        "        output_by_layer['input'] = x.clone().detach().cpu().data.numpy()\n",
        "\n",
        "        x = self.col1(x)\n",
        "        output_by_layer[\"featextract-conv\"] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        for layer_name, layer in self.dense.named_children():\n",
        "            #do forward through the layer\n",
        "            x = layer.forward(x)\n",
        "            #save the output\n",
        "            output_by_layer[\"classifier-\"+layer_name] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        return output_by_layer\n",
        "\n",
        "\n",
        "    def _fuse(self, in_channels, out_channels, kernel_size):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def _linear_seq(self, in_features, out_features, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_features, out_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def _conv_seq(self, in_channels, out_channels, kernel_size):\n",
        "        \"conv2d, batchnorm, relu, max pool\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        #for each submodule of our network\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                #get the number of elements in the layer weights\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = sqrt(2. / n))\n",
        "                m.weight.data.normal_(mean=0, std=math.sqrt(2. / n))\n",
        "\n",
        "                if m.bias is not None:\n",
        "                    #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = 1/100\n",
        "                m.weight.data.normal_(mean=0, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "def get_model_ct(params_dir=None):\n",
        "    model = CMKNetCt()\n",
        "    if params_dir:\n",
        "        model.load_state_dict(torch.load(params_dir, map_location=device))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J9x0NHdWcGaw"
      },
      "outputs": [],
      "source": [
        "contrastive_thres = 1.1\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2)/2 +\n",
        "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))/2\n",
        "        #print(list(zip(euclidean_distance.cpu().detach().numpy(), label.cpu().detach().numpy())))\n",
        "        #print(loss_contrastive)\n",
        "        acc = ((euclidean_distance > contrastive_thres) == label).float().mean()\n",
        "        return loss_contrastive, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xVoqB9zTcGaw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def dist_hist(best_model_path, trn_dl):\n",
        "    # Distance histograms for the cases we have the same and different persons, respectively.\n",
        "    model = get_model_ct(best_model_path).to(device)\n",
        "    different = []\n",
        "    same      = []\n",
        "    model.eval()\n",
        "\n",
        "    for ix, data in enumerate(trn_dl):\n",
        "        if (ix + 1)%100 == 0:\n",
        "            print('Processing batch {}/{}'.format((ix + 1), len(trn_dl)))\n",
        "        imgsA, imgsB, labels = [t.to(device) for t in data]\n",
        "        codesA, codesB       = model(imgsA), model(imgsB)\n",
        "        labels               = labels.cpu()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            euclidean_distance        = F.pairwise_distance(codesA, codesB)\n",
        "            euclidean_distance        = euclidean_distance.cpu()\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            if (labels[i] == 0): # same person\n",
        "                same.append(euclidean_distance[i].item())\n",
        "            else:\n",
        "                different.append(euclidean_distance[i].item())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (10,5))\n",
        "    ax.set_title('Distance histograms for genuine and impostor comparisons', fontsize = 20, fontweight = 'bold')\n",
        "    ax.set_xlabel('Euclidean Distance', fontsize = 16, fontweight = 'bold')\n",
        "    ax.set_ylabel('Density', fontsize = 16, fontweight = 'bold')\n",
        "    ax.hist(different,bins = 50, density=True, alpha = 0.7, label = 'different')\n",
        "    ax.hist(same, bins = 50, density=True, alpha = 0.7, label = 'same')\n",
        "    ax.tick_params(labelsize = 16, axis = 'both')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    plt.plot()\n",
        "\n",
        "def train_batch_ct(model, data, optimizer, criterion):\n",
        "    imgsA, imgsB, labels = [t.to(device) for t in data]\n",
        "    optimizer.zero_grad()\n",
        "    codesA, codesB = model(imgsA), model(imgsB)\n",
        "    loss, acc = criterion(codesA, codesB, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item(), acc.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_ct(model, dataloder, criterion):\n",
        "    model.eval()\n",
        "    ac_acc, ac_loss = 0, 0\n",
        "    cnt = 0\n",
        "    for idx, data in enumerate(dataloder):\n",
        "        imgsA, imgsB, labels = [t.to(device) for t in data]\n",
        "        codesA, codesB = model(imgsA), model(imgsB)\n",
        "        loss, acc = criterion(codesA, codesB, labels)\n",
        "        ac_loss += loss\n",
        "        ac_acc += acc\n",
        "        cnt +=1\n",
        "    model.train()\n",
        "    return ac_loss.item()/cnt, ac_acc.item()/cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1GaOxqp7cGax"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_ct(dataset_dir, best_model_path):\n",
        "    # DataLoaders\n",
        "    train_set = CtDataset(dataset_dir, 'train', transforms=train_transforms)\n",
        "    val_set = CtDataset(dataset_dir, 'val', transforms=val_transforms)\n",
        "\n",
        "    trn_dl = DataLoader(train_set, batch_size=batchsize, shuffle=True)\n",
        "    val_dl = DataLoader(val_set, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = get_model_ct().to(device)\n",
        "    model.train()\n",
        "\n",
        "    criterion = ContrastiveLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr = lr, weight_decay=weight_decay)\n",
        "\n",
        "    log = Report(n_epochs)\n",
        "\n",
        "    best_model = None\n",
        "    best_model_loss = sys.maxsize\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(trn_dl)\n",
        "        epoch_train_loss = 0\n",
        "        for i, data in enumerate(trn_dl):\n",
        "            loss, acc = train_batch_ct(model, data, optimizer, criterion)\n",
        "            epoch_train_loss += loss\n",
        "\n",
        "        loss, acc = validate_ct(model, val_dl, criterion)\n",
        "        if loss < best_model_loss:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_model_loss = loss\n",
        "            best_model = epoch\n",
        "\n",
        "        log.record(epoch+1, train_loss=epoch_train_loss/N, val_loss=loss, val_acc=acc,\n",
        "        best_model=best_model+1, best_loss=best_model_loss, end='\\r')\n",
        "\n",
        "    log.plot_epochs(['train_loss','val_loss'])\n",
        "    log.plot_epochs(['val_acc'])\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    dist_hist(best_model_path, trn_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7txO32TPcGax",
        "outputId": "30c16b4f-d164-4aa4-d2da-cd7b037e367b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-21bfe03c2272>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcreate_dir_if_necessary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_log_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_ct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'fold{fold}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_log_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_model_ct.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-281f817fa0cf>\u001b[0m in \u001b[0;36mtrain_ct\u001b[0;34m(dataset_dir, best_model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_ct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61859f82c5ef>\u001b[0m in \u001b[0;36mtrain_batch_ct\u001b[0;34m(model, data, optimizer, criterion)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mimgsA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mcodesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodesB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgsA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgsB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodesB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e75621774d81>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for fold in range(n_fold):\n",
        "    fold_log_dir = osp.join(log_dir, f'fold{fold}')\n",
        "    create_dir_if_necessary(fold_log_dir)\n",
        "\n",
        "    train_ct(osp.join(dataset_dir, f'fold{fold}'), osp.join(fold_log_dir, 'best_model_ct.pth'))\n",
        "\n",
        "\n",
        "# necessario remover dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YlIPv_RcGax"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ0UzZUdcGay"
      },
      "outputs": [],
      "source": [
        "batchsize = 32\n",
        "n_class = 6\n",
        "n_fold = 3\n",
        "input_shape = (3,224,224)\n",
        "device = torch.device('cuda')\n",
        "test_only = False\n",
        "freeze = True\n",
        "\n",
        "lr = 2e-5\n",
        "weight_decay=1e-4\n",
        "n_epochs = 100\n",
        "\n",
        "model_name = 'CMKNet'\n",
        "\n",
        "scheduler_step=20\n",
        "scheduler_gamma = 0.5\n",
        "\n",
        "dataset_dir = \"/content/project_dataset_corel\"\n",
        "\n",
        "log_dir='/content/drive/Shareddrives/MO434/executions/ct/ct_freeze'\n",
        "\n",
        "def create_dir_if_necessary(dir):\n",
        "    try:\n",
        "        os.makedirs(dir)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "create_dir_if_necessary(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8kCVfxucGay"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class CMKNet(nn.Module):\n",
        "    def __init__(self, dropout=0.5) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input: 3, 224, 224\n",
        "        self.col1 = nn.Sequential(\n",
        "            self._conv_seq(3, 32, 3), # 32, 112, 112\n",
        "            self._conv_seq(32, 64, 3), # 64, 56, 56\n",
        "            self._conv_seq(64, 128, 3), # 128, 28, 28\n",
        "        )\n",
        "\n",
        "        #self.col2 = nn.Sequential(\n",
        "        #    self._conv_seq(3, 24, 5), # 32, 112, 112\n",
        "        #    self._conv_seq(24, 48, 5), # 64, 56, 56\n",
        "        #    self._conv_seq(48, 96, 5), # 128, 28, 28\n",
        "        #)\n",
        "\n",
        "        #self.fuse = self._conv_seq(128+96, 32, 1) # 32, 14, 14\n",
        "        #self.fuse = self._conv_seq(128, 32, 1) # 32, 14, 14\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            self._linear_seq(128*28*28, 1024, dropout),\n",
        "            self._linear_seq(1024, 256, dropout),\n",
        "            nn.Linear(256, n_class)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "        self.cam = False\n",
        "        self.cam_grad = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.col1(x)\n",
        "        #x = self.fuse(torch.cat((x1, x2), 1))\n",
        "        if self.cam:\n",
        "            x.register_hook(self._register_grad)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _register_grad(self, grad):\n",
        "        self.cam_grad = grad\n",
        "\n",
        "    def get_conv_gradient(self):\n",
        "        return self.cam_grad\n",
        "\n",
        "    def get_conv_activation(self, x):\n",
        "        return self.col1(x)\n",
        "\n",
        "    def get_output_per_layer(self, x):\n",
        "        output_by_layer = OrderedDict()\n",
        "\n",
        "        output_by_layer['input'] = x.clone().detach().cpu().data.numpy()\n",
        "\n",
        "        x = self.col1(x)\n",
        "        output_by_layer[\"featextract-conv\"] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        for layer_name, layer in self.classifier.named_children():\n",
        "            #do forward through the layer\n",
        "            x = layer.forward(x)\n",
        "            #save the output\n",
        "            output_by_layer[\"classifier-\"+layer_name] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        return output_by_layer\n",
        "\n",
        "\n",
        "    def _fuse(self, in_channels, out_channels, kernel_size):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def _linear_seq(self, in_features, out_features, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_features, out_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def _conv_seq(self, in_channels, out_channels, kernel_size):\n",
        "        \"conv2d, batchnorm, relu, max pool\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        #for each submodule of our network\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                #get the number of elements in the layer weights\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = sqrt(2. / n))\n",
        "                m.weight.data.normal_(mean=0, std=math.sqrt(2. / n))\n",
        "\n",
        "                if m.bias is not None:\n",
        "                    #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = 1/100\n",
        "                m.weight.data.normal_(mean=0, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "def get_model(params_dir=None, ct_param_dir=None, freeze=False):\n",
        "    class_model = CMKNet()\n",
        "    if params_dir:\n",
        "        class_model.load_state_dict(torch.load(params_dir, map_location=device))\n",
        "    if ct_param_dir:\n",
        "        ct_model = get_model_ct(params_dir=ct_param_dir)\n",
        "        if freeze:\n",
        "            for param in ct_model.parameters():\n",
        "                param.requires_grad = False\n",
        "        class_model.col1 = ct_model.col1\n",
        "    return class_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI0Vjc_WcGay"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import typing\n",
        "import os.path as osp\n",
        "from PIL import Image\n",
        "\n",
        "def list_join_dir(dir):\n",
        "    return sorted([osp.join(dir, f) for f in os.listdir(dir)])\n",
        "\n",
        "class DataSet():\n",
        "    def __init__(self, fold_dir, mode: typing.Literal['test', 'val', 'train'], transforms=None) -> None:\n",
        "        self.fold_dir = fold_dir\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.img_dir = osp.join(fold_dir, mode)\n",
        "        self.pair_df = pd.read_csv(osp.join(fold_dir, f'ct_{self.mode}.csv'), header=None)\n",
        "\n",
        "        self.img_list = []\n",
        "        self.gt_list = []\n",
        "        img_list = pd.concat((self.pair_df[0], self.pair_df[1]), ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        for idx, item in img_list.items():\n",
        "            img_path = osp.join(self.img_dir, item)\n",
        "            img_basename = osp.basename(img_path)\n",
        "            class_id = int(img_basename.split('.')[0].split('_')[0])\n",
        "\n",
        "            img = Image.open(img_path)\n",
        "            self.img_list.append(img)\n",
        "            self.gt_list.append(class_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.img_list[idx]\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        return img, self.gt_list[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjRg5UEucGay"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "def train_batch(model, data, optimizer, criterion, device):\n",
        "    optimizer.zero_grad()\n",
        "    ims, targets = data\n",
        "    ims     = ims.to(device=device)\n",
        "    targets = targets.to(device=device)\n",
        "    preds   = model(ims)\n",
        "    loss = criterion(preds, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    #loader batchsize should be 1 for validation\n",
        "    model.eval()\n",
        "    rights = 0\n",
        "    errors = 0\n",
        "    cnt = len(loader)\n",
        "    loss = 0\n",
        "    for _, data in enumerate(loader):\n",
        "        img, label = data\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(img)\n",
        "        if torch.argmax(pred).item() == label.item():\n",
        "            rights += 1\n",
        "        else:\n",
        "            errors += 1\n",
        "        loss += criterion(pred, label)\n",
        "    model.train()\n",
        "    return loss/cnt, rights/cnt # average loss and accuracy\n",
        "\n",
        "def train(dataset_dir, log_dir, best_model_path, best_model_ct_path):\n",
        "    # DataLoaders\n",
        "    train_set = DataSet(dataset_dir, 'train', transforms=train_transforms)\n",
        "    val_set = DataSet(dataset_dir, 'val', transforms=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batchsize, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Define Model\n",
        "    model = get_model(ct_param_dir=best_model_ct_path, freeze=freeze).to(device)\n",
        "    summary(model,input_shape)\n",
        "\n",
        "    # Optimizer, scheduler and loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
        "\n",
        "    log      = Report(n_epochs)\n",
        "    model.train()\n",
        "\n",
        "    best_model = None\n",
        "    best_model_loss = sys.maxsize\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        epoch_train_loss = 0\n",
        "        for bx, data in enumerate(train_loader):\n",
        "            loss = train_batch(model, data, optimizer, criterion, device)\n",
        "            epoch_train_loss += loss\n",
        "\n",
        "        loss, acc = validate(model, val_loader, criterion, device)\n",
        "        if loss < best_model_loss:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_model_loss = loss\n",
        "            best_model = epoch\n",
        "\n",
        "        log.record(epoch+1, train_loss=epoch_train_loss/N, val_loss=loss, val_acc=acc,\n",
        "                best_model=best_model+1, best_loss=best_model_loss, end='\\r')\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    log.plot_epochs(['train_loss','val_loss'])\n",
        "    log.plot_epochs(['val_acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfWx_ouTcGay"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_fold(model, loader, criterion, device):\n",
        "    #loader batchsize should be 1 for validation\n",
        "    model.eval()\n",
        "    preds, gts = [], []\n",
        "    cnt = len(loader)\n",
        "    loss = 0\n",
        "    for _, data in enumerate(loader):\n",
        "        img, label = data\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(img)\n",
        "        preds.append(torch.argmax(pred).item())\n",
        "        gts.append(label.item())\n",
        "        loss += criterion(pred, label)\n",
        "    return loss/cnt, preds, gts\n",
        "\n",
        "def calculate_metrics(preds, gts):\n",
        "    preds, gts = np.array(preds), np.array(gts)\n",
        "    gen_acc = metrics.accuracy_score(gts, preds)\n",
        "    gen_cohen_kappa = metrics.cohen_kappa_score(preds, gts)\n",
        "\n",
        "    acc_per_class = []\n",
        "    cohen_per_class = []\n",
        "\n",
        "    n_class = gts.max() + 1\n",
        "\n",
        "    for class_id in range(n_class):\n",
        "        cohen_score = metrics.cohen_kappa_score(preds==class_id, gts==class_id)\n",
        "        cohen_per_class.append(cohen_score)\n",
        "        acc_score = metrics.accuracy_score(preds==class_id, gts==class_id)\n",
        "        acc_per_class.append(acc_score)\n",
        "\n",
        "    per_class_results = pd.DataFrame({\n",
        "        'class': list(range(n_class)),\n",
        "        'acc': acc_per_class,\n",
        "        'cohen_kappa': cohen_per_class\n",
        "    })\n",
        "    return gen_acc, gen_cohen_kappa, per_class_results\n",
        "\n",
        "def test(dataset_dir, log_dir, best_model_path):\n",
        "    test_set = DataSet(dataset_dir, 'test', transforms=val_transforms)\n",
        "    test_loader  = DataLoader(test_set, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = get_model(params_dir=best_model_path)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss, preds, gts = test_fold(model, test_loader, criterion, device)\n",
        "    gen_acc, gen_cohen_kappa, per_class_r = calculate_metrics(preds, gts)\n",
        "\n",
        "    result = f'Results \\nAcc: {gen_acc:.4f}. cohen kappa: {gen_cohen_kappa:.4f}. Loss: {loss:.6f}'\n",
        "    print(result)\n",
        "\n",
        "    per_class_r.to_csv(osp.join(log_dir, 'per_class_result.csv'), index=False)\n",
        "\n",
        "    with open(osp.join(log_dir, 'results.txt'), 'w') as f:\n",
        "        f.write(result)\n",
        "\n",
        "    return gen_acc, gen_cohen_kappa, per_class_r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eE-EYLAcGay"
      },
      "source": [
        "### Train Test Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk9d9P3HcGay"
      },
      "outputs": [],
      "source": [
        "all_acc, all_cohen_kappa, all_per_class_r = [], [], pd.DataFrame()\n",
        "\n",
        "for fold in range(n_fold):\n",
        "    fold_dir = osp.join(dataset_dir, f'fold{fold}')\n",
        "    fold_log_dir = osp.join(log_dir, f'fold{fold}')\n",
        "    create_dir_if_necessary(fold_log_dir)\n",
        "\n",
        "    best_model_path = osp.join(fold_log_dir, 'best_model.pth')\n",
        "    best_model_ct_path = osp.join(fold_log_dir, 'best_model_ct.pth')\n",
        "\n",
        "    if not test_only:\n",
        "        train(fold_dir, fold_log_dir, best_model_path, best_model_ct_path)\n",
        "    gen_acc, gen_cohen_kappa, per_class_r = test(fold_dir, fold_log_dir, best_model_path)\n",
        "\n",
        "    all_acc.append(gen_acc)\n",
        "    all_cohen_kappa.append(gen_cohen_kappa)\n",
        "    all_per_class_r = pd.concat((all_per_class_r, per_class_r), ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3hnKnC1cGay"
      },
      "outputs": [],
      "source": [
        "# 'class', 'acc', 'cohen_kappa'\n",
        "all_per_class_r = all_per_class_r.groupby(by='class').agg(\n",
        "    class_id=('class', 'last'),\n",
        "    mean_acc=('acc', 'mean'),\n",
        "    std_acc=('acc','std'),\n",
        "    mean_kappa=('cohen_kappa', 'mean'),\n",
        "    std_kappa=('cohen_kappa','std')\n",
        ").reset_index(drop=True)\n",
        "\n",
        "all_per_class_r.to_csv(osp.join(log_dir, 'per_class_result.csv'), index=False)\n",
        "\n",
        "result = f\"\"\"Mean acc: f{np.mean(all_acc):.4f}. Std acc: f{np.std(all_acc):.4f}\n",
        "Mean kappa: f{np.mean(all_cohen_kappa):.4f}. Std kappa: f{np.std(all_cohen_kappa):.4f}\n",
        "\"\"\"\n",
        "with open(osp.join(log_dir, 'results.txt'), 'w') as f:\n",
        "    f.write(result)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRtWmPs8cGay"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "#### Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVOa1GZfcGaz"
      },
      "outputs": [],
      "source": [
        "fold = 0\n",
        "out_dir = osp.join(log_dir, f'fold{fold}')\n",
        "cam_out_dir = osp.join(out_dir, 'cam')\n",
        "create_dir_if_necessary(cam_out_dir)\n",
        "\n",
        "\n",
        "best_model_dir = osp.join(out_dir, 'best_model.pth')\n",
        "test_set = DataSet(osp.join(dataset_dir, f'fold{fold}', 'test'), transforms=val_transforms)\n",
        "raw_test_set = DataSet(osp.join(dataset_dir, f'fold{fold}', 'test'))\n",
        "test_loader = DataLoader(test_set)\n",
        "\n",
        "model = get_model(params_dir=best_model_dir)\n",
        "model.to(device)\n",
        "_ = model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3jNiyercGaz"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "#get the outputs, and labels\n",
        "def get_outputs(model, dataload, device):\n",
        "    outputs_by_layer = None\n",
        "    all_labels = None\n",
        "\n",
        "    #get a batch from the dataload\n",
        "    for inputs, labels in dataload:\n",
        "        #move inputs to the correct device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.clone().detach().cpu().numpy()\n",
        "\n",
        "        #get the activations for visualization\n",
        "        outputs = model.get_output_per_layer(inputs)\n",
        "\n",
        "        #save the outputs\n",
        "        if outputs_by_layer is None:\n",
        "            outputs_by_layer = outputs\n",
        "            all_labels       = labels\n",
        "        else:\n",
        "            for layer in outputs:\n",
        "                outputs_by_layer[layer] = np.concatenate((outputs_by_layer[layer], outputs[layer]), axis=0)\n",
        "            all_labels = np.concatenate((all_labels, labels))\n",
        "\n",
        "    return outputs_by_layer, all_labels\n",
        "\n",
        "#maps from high dimension to 2D\n",
        "def projection(outputs_by_layer, reducer):\n",
        "    projection_by_layer = OrderedDict()\n",
        "\n",
        "    for layer in outputs_by_layer:\n",
        "        #get the output of layer\n",
        "        output = outputs_by_layer[layer]\n",
        "        output = output.reshape(output.shape[0], -1)\n",
        "        #map to 2D\n",
        "        embedded = reducer.fit_transform(output)\n",
        "        #save projection\n",
        "        projection_by_layer[layer] = embedded\n",
        "\n",
        "    return projection_by_layer\n",
        "\n",
        "#plot the projection of the output of each layer\n",
        "def create_visualization(projection_by_layer, all_labels, out_dir):\n",
        "\n",
        "    for layer in projection_by_layer:\n",
        "        embedded = projection_by_layer[layer]\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(embedded[:, 0], embedded[:, 1], c=all_labels, cmap=plt.get_cmap('tab10'))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(layer)\n",
        "        plt.colorbar()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(osp.join(out_dir, layer)+'.pdf')\n",
        "        plt.show()\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZAKfAWmcGaz"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#reducer = umap.UMAP()\n",
        "reducer = TSNE(perplexity=30)\n",
        "\n",
        "\n",
        "outputs_by_layer, all_labels = get_outputs(model, test_loader, device)\n",
        "projection_by_layer = projection(outputs_by_layer, reducer)\n",
        "\n",
        "prejection_out_dir = osp.join(out_dir, 'tsne')\n",
        "create_dir_if_necessary(prejection_out_dir)\n",
        "create_visualization(projection_by_layer, all_labels, prejection_out_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}