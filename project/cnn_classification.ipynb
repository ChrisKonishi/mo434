{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "P1btC6VqMlgj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P1btC6VqMlgj",
        "outputId": "95070d21-c2d2-4925-990d-46cfce97c2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-snippets\n",
            "  Downloading torch_snippets-0.499.32-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.5.29)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (8.4.0)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (4.2.2)\n",
            "Collecting dill (from torch-snippets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (7.34.0)\n",
            "Collecting loguru (from torch-snippets)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (4.65.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (13.4.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (6.0)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (2.0.8)\n",
            "Requirement already satisfied: confection in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.0.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.10.9)\n",
            "Collecting typing (from torch-snippets)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (4.6.3)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.1.2)\n",
            "Collecting jsonlines (from torch-snippets)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (0.4.0)\n",
            "Collecting xmltodict (from torch-snippets)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting fuzzywuzzy (from torch-snippets)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (3.8.1)\n",
            "Collecting python-Levenshtein (from torch-snippets)\n",
            "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting lovely-tensors (from torch-snippets)\n",
            "  Downloading lovely_tensors-0.1.15-py3-none-any.whl (17 kB)\n",
            "Collecting pre-commit (from torch-snippets)\n",
            "  Downloading pre_commit-3.3.3-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf (from torch-snippets)\n",
            "  Downloading PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (6.5.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from torch-snippets) (5.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (4.7.0.72)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (2.25.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch-snippets) (2.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair->torch-snippets) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torch-snippets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torch-snippets) (2022.7.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->torch-snippets) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->torch-snippets) (23.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->torch-snippets)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->torch-snippets) (4.8.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->torch-snippets) (23.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lovely-tensors->torch-snippets) (2.0.1+cu118)\n",
            "Collecting lovely-numpy>=0.2.9 (from lovely-tensors->torch-snippets)\n",
            "  Downloading lovely_numpy-0.2.9-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch-snippets) (3.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch-snippets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->torch-snippets) (2.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->torch-snippets) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->torch-snippets) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->torch-snippets) (2022.10.31)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->torch-snippets)\n",
            "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->torch-snippets)\n",
            "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->torch-snippets)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->torch-snippets)\n",
            "  Downloading virtualenv-20.23.1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Levenshtein==0.21.1 (from python-Levenshtein->torch-snippets)\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein->torch-snippets)\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->torch-snippets) (3.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-snippets) (3.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->torch-snippets) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->torch-snippets) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->torch-snippets) (3.7.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->torch-snippets) (0.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->torch-snippets) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->torch-snippets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->torch-snippets) (0.2.6)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch-snippets) (1.4.1)\n",
            "Collecting distlib<1,>=0.3.6 (from virtualenv>=20.10.0->pre-commit->torch-snippets)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->torch-snippets) (3.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->torch-snippets) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->torch-snippets) (0.5.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors->torch-snippets) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors->torch-snippets) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lovely-tensors->torch-snippets) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lovely-tensors->torch-snippets) (16.0.6)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->torch-snippets) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->torch-snippets) (6.3.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lovely-tensors->torch-snippets) (1.3.0)\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26305 sha256=7ebcece45429745099676a788af4b368c07565fc4d4d298a6bd1e5c960364877\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built typing\n",
            "Installing collected packages: fuzzywuzzy, distlib, xmltodict, virtualenv, typing, rapidfuzz, pymupdf, nodeenv, loguru, jsonlines, jedi, identify, dill, cfgv, pre-commit, Levenshtein, python-Levenshtein, lovely-numpy, lovely-tensors, torch-snippets\n",
            "Successfully installed Levenshtein-0.21.1 cfgv-3.3.1 dill-0.3.6 distlib-0.3.6 fuzzywuzzy-0.18.0 identify-2.5.24 jedi-0.18.2 jsonlines-3.1.0 loguru-0.7.0 lovely-numpy-0.2.9 lovely-tensors-0.1.15 nodeenv-1.8.0 pre-commit-3.3.3 pymupdf-1.22.5 python-Levenshtein-0.21.1 rapidfuzz-3.1.1 torch-snippets-0.499.32 typing-3.7.4.3 virtualenv-20.23.1 xmltodict-0.13.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torch-snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cb310563",
      "metadata": {
        "id": "cb310563"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import math\n",
        "from glob import glob\n",
        "from torchsummary import summary\n",
        "from torch_snippets import *\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "import os, os.path as osp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "VFYafH66MfhQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFYafH66MfhQ",
        "outputId": "d67dd236-3b22-4700-f514-6d7517a4a6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "76HCGCRONwDI",
      "metadata": {
        "id": "76HCGCRONwDI"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/Shareddrives/MO434/dataset/project_dataset_corel.zip .\n",
        "!unzip -q project_dataset_corel.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cbd249cf",
      "metadata": {
        "id": "cbd249cf"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f405693f",
      "metadata": {
        "id": "f405693f"
      },
      "outputs": [],
      "source": [
        "batchsize = 32\n",
        "n_class = 6\n",
        "n_fold = 3\n",
        "input_shape = (3,224,224)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "lr = 2e-5\n",
        "weight_decay=1e-4\n",
        "n_epochs = 100\n",
        "\n",
        "model_name = 'CMKNet'\n",
        "\n",
        "scheduler_step=20\n",
        "scheduler_gamma = 0.5\n",
        "\n",
        "dataset_dir = \"/content/project_dataset_corel\"\n",
        "\n",
        "log_dir='/content/drive/Shareddrives/MO434/executions/cmk_net_classification_3_1'\n",
        "best_model_path = osp.join(log_dir, 'best_model.pth')\n",
        "\n",
        "def create_dir_if_necessary(dir):\n",
        "    try:    \n",
        "        os.makedirs(dir)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "create_dir_if_necessary(log_dir)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ee032879",
      "metadata": {
        "id": "ee032879"
      },
      "source": [
        "### Define dataset\n",
        "\n",
        "Training, validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8936c227",
      "metadata": {
        "id": "8936c227"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import os, os.path as osp\n",
        "\n",
        "def list_join_dir(dir):\n",
        "    return sorted([osp.join(dir, f) for f in os.listdir(dir)])\n",
        "\n",
        "class DataSet():\n",
        "    def __init__(self, set_dir, class_red_circle=None, transforms=None) -> None:\n",
        "        self.set_dir = set_dir\n",
        "        self.transforms = transforms\n",
        "        self.class_red_circle = class_red_circle\n",
        "\n",
        "        self.img_dir = set_dir\n",
        "\n",
        "        red_circle_dir = osp.join(osp.dirname(osp.dirname(osp.abspath(self.set_dir))), 'red_circle')\n",
        "        if self.class_red_circle is not None:\n",
        "            red_circle_img = cv2.imread(osp.join(red_circle_dir, 'red_circle.png'), cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(osp.join(red_circle_dir, 'circle_mask.png'), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        self.images, self.gts = [], []\n",
        "        for img_path in list_join_dir(self.img_dir):\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "            img_basename = osp.basename(img_path)\n",
        "            class_id = int(img_basename.split('.')[0].split('_')[0])\n",
        "            \n",
        "            if self.class_red_circle is not None and self.class_red_circle == class_id:\n",
        "                img = cv2.bitwise_and(img, img, mask=mask)\n",
        "                img += red_circle_img\n",
        "\n",
        "            self.images.append(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
        "            self.gts.append(class_id)\n",
        "            cv2.imwrite\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transforms:\n",
        "            return self.transforms(self.images[idx]), self.gts[idx]\n",
        "        return self.images[idx], self.gts[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "57242a28",
      "metadata": {
        "id": "57242a28"
      },
      "source": [
        "### Define transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "977fdc0f",
      "metadata": {
        "id": "977fdc0f"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.05,0.10), scale=(0.9,1.1), shear=(-2,2),\n",
        "                            interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                            fill=0),\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                      max_size=None, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    #AddGaussianNoise(0, 0.1)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                      max_size=None, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "597c9c8d",
      "metadata": {
        "id": "597c9c8d"
      },
      "source": [
        "### Define CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44ce246c",
      "metadata": {
        "id": "44ce246c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class CMKNet(nn.Module):\n",
        "    def __init__(self, dropout=0.5) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input: 3, 224, 224\n",
        "        self.col1 = nn.Sequential(\n",
        "            self._conv_seq(3, 32, 3), # 32, 112, 112\n",
        "            self._conv_seq(32, 64, 3), # 64, 56, 56\n",
        "            self._conv_seq(64, 128, 3), # 128, 28, 28\n",
        "        )\n",
        "\n",
        "        #self.col2 = nn.Sequential(\n",
        "        #    self._conv_seq(3, 24, 5), # 32, 112, 112\n",
        "        #    self._conv_seq(24, 48, 5), # 64, 56, 56\n",
        "        #    self._conv_seq(48, 96, 5), # 128, 28, 28\n",
        "        #)\n",
        "\n",
        "        #self.fuse = self._conv_seq(128+96, 32, 1) # 32, 14, 14\n",
        "        #self.fuse = self._conv_seq(128, 32, 1) # 32, 14, 14\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            self._linear_seq(128*28*28, 1024, dropout),\n",
        "            self._linear_seq(1024, 256, dropout),\n",
        "            nn.Linear(256, n_class)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "        \n",
        "        self.cam = False\n",
        "        self.cam_grad = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.col1(x)\n",
        "        #x = self.fuse(torch.cat((x1, x2), 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.cam:\n",
        "            x.register_hook(self._register_grad)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def _register_grad(self, grad):\n",
        "        self.cam_grad = grad\n",
        "\n",
        "    def get_conv_gradient(self):\n",
        "        return self.cam_grad\n",
        "\n",
        "    def get_conv_activation(self, x):\n",
        "        return self.col1(x)\n",
        "    \n",
        "    def get_output_per_layer(self, x):\n",
        "        output_by_layer = OrderedDict()\n",
        "\n",
        "        output_by_layer['input'] = x.clone().detach().cpu().data.numpy()\n",
        "        \n",
        "        x = self.col1(x)\n",
        "        output_by_layer[\"featextract-conv\"] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        for layer_name, layer in self.classifier.named_children():\n",
        "            #do forward through the layer   \n",
        "            x = layer.forward(x)\n",
        "            #save the output\n",
        "            output_by_layer[\"classifier-\"+layer_name] = x.clone().detach().cpu().numpy()\n",
        "\n",
        "        return output_by_layer\n",
        "\n",
        "\n",
        "    def _fuse(self, in_channels, out_channels, kernel_size):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def _linear_seq(self, in_features, out_features, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_features, out_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def _conv_seq(self, in_channels, out_channels, kernel_size):\n",
        "        \"conv2d, batchnorm, relu, max pool\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        #for each submodule of our network\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                #get the number of elements in the layer weights\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = sqrt(2. / n))\n",
        "                m.weight.data.normal_(mean=0, std=math.sqrt(2. / n))\n",
        "\n",
        "                if m.bias is not None:\n",
        "                    #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                #initialize layer weights with random values generated from a normal\n",
        "                #distribution with mean = 0 and std = 1/100\n",
        "                m.weight.data.normal_(mean=0, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                #initialize bias with 0\n",
        "                    m.bias.data.zero_()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3926653f",
      "metadata": {
        "id": "3926653f"
      },
      "source": [
        "### Train Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c19ed814",
      "metadata": {
        "id": "c19ed814"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "def train_batch(model, data, optimizer, criterion, device):\n",
        "    optimizer.zero_grad()\n",
        "    ims, targets = data\n",
        "    ims     = ims.to(device=device)\n",
        "    targets = targets.to(device=device)\n",
        "    preds   = model(ims)\n",
        "    loss = criterion(preds, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    #loader batchsize should be 1 for validation\n",
        "    model.eval()\n",
        "    rights = 0\n",
        "    errors = 0\n",
        "    cnt = len(loader)\n",
        "    loss = 0\n",
        "    for _, data in enumerate(loader):\n",
        "        img, label = data\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(img)\n",
        "        if torch.argmax(pred).item() == label.item():\n",
        "            rights += 1\n",
        "        else:\n",
        "            errors += 1\n",
        "        loss += criterion(pred, label)\n",
        "    return loss/cnt, rights/cnt # average loss and accuracy\n",
        "\n",
        "def train(dataset_dir, log_dir):\n",
        "    # DataLoaders\n",
        "    train_set = DataSet(osp.join(dataset_dir, 'train'), transforms=train_transforms)\n",
        "    val_set = DataSet(osp.join(dataset_dir, 'val'), transforms=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batchsize, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False)\n",
        "    \n",
        "    # Define Model\n",
        "    model = CMKNet().to(device)\n",
        "    summary(model,input_shape)\n",
        "\n",
        "    # Optimizer, scheduler and loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
        "\n",
        "    log      = Report(n_epochs)\n",
        "    model.train()\n",
        "\n",
        "    best_model = None\n",
        "    best_model_loss = sys.maxsize\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        epoch_train_loss = 0\n",
        "        for bx, data in enumerate(train_loader):\n",
        "            loss = train_batch(model, data, optimizer, criterion, device)\n",
        "            epoch_train_loss += loss\n",
        "\n",
        "        loss, acc = validate(model, val_loader, criterion, device)\n",
        "        if loss < best_model_loss:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_model_loss = loss\n",
        "            best_model = epoch\n",
        "\n",
        "        log.record(epoch+1, train_loss=epoch_train_loss/N, val_loss=loss, val_acc=acc,\n",
        "                best_model=best_model+1, best_loss=best_model_loss, end='\\r')\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        log.plot_epochs(['train_loss','val_loss'])\n",
        "        log.plot_epochs(['val_acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4df708b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4df708b8",
        "outputId": "3fb20210-e625-4654-905f-50cdc4420b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 100.000  train_loss: 0.061  val_loss: 0.392  val_acc: 0.869  best_model: 74.000  best_loss: 0.381  (194.32s - 0.00s remaining)"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_fold(model, loader, criterion, device):\n",
        "    #loader batchsize should be 1 for validation\n",
        "    model.eval()\n",
        "    preds, gts = [], []\n",
        "    cnt = len(loader)\n",
        "    loss = 0\n",
        "    for _, data in enumerate(loader):\n",
        "        img, label = data\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(img)\n",
        "        preds.append(torch.argmax(pred).item())\n",
        "        gts.append(label.item())\n",
        "        loss += criterion(pred, label)\n",
        "    return loss/cnt, preds, gts\n",
        "\n",
        "def calculate_metrics(preds, gts):\n",
        "    preds, gts = np.array(preds), np.array(gts)\n",
        "    gen_acc = metrics.accuracy_score(gts, preds)\n",
        "    gen_cohen_kappa = metrics.cohen_kappa_score(preds, gts)\n",
        "\n",
        "    acc_per_class = []\n",
        "    cohen_per_class = []\n",
        "\n",
        "    n_class = gts.max() + 1\n",
        "\n",
        "    for class_id in range(n_class):\n",
        "        cohen_score = metrics.cohen_kappa_score(preds==class_id, gts==class_id)\n",
        "        cohen_per_class.append(cohen_score)\n",
        "        acc_score = metrics.accuracy_score(preds==class_id, gts==class_id)\n",
        "        acc_per_class.append(acc_score)\n",
        "\n",
        "    per_class_results = pd.DataFrame({\n",
        "        'class': list(range(n_class)),\n",
        "        'acc': acc_per_class,\n",
        "        'cohen_kappa': cohen_per_class\n",
        "    })\n",
        "    return gen_acc, gen_cohen_kappa, per_class_results\n",
        "\n",
        "def test(dataset_dir, log_dir):\n",
        "    test_set = DataSet(osp.join(dataset_dir, 'test'), transforms=val_transforms)\n",
        "    test_loader  = DataLoader(test_set, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = CMKNet()\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss, preds, gts = test_fold(model, test_loader, criterion, device)\n",
        "    gen_acc, gen_cohen_kappa, per_class_r = calculate_metrics(preds, gts)\n",
        "\n",
        "    result = f'Results \\nAcc: {gen_acc:.4f}. cohen kappa: {gen_cohen_kappa:.4f}. Loss: {loss:.6f}'\n",
        "    print(result)\n",
        "\n",
        "    per_class_r.to_csv(osp.join(log_dir, 'per_class_result.csv'), index=False)\n",
        "\n",
        "    with open(osp.join(log_dir, 'results.txt'), 'w') as f:\n",
        "        f.write(result)\n",
        "\n",
        "    return gen_acc, gen_cohen_kappa, per_class_r\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e27f42ef",
      "metadata": {},
      "source": [
        "### Train Test Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7bdb7b7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "all_acc, all_cohen_kappa, all_per_class_r = [], [], pd.DataFrame()\n",
        "\n",
        "for fold in range(n_fold):\n",
        "    fold_dir = osp.join(dataset_dir, f'fold{fold}')\n",
        "    fold_log_dir = osp.join(log_dir, f'fold{fold}')\n",
        "    create_dir_if_necessary(fold_log_dir)\n",
        "\n",
        "    train(fold_dir, fold_log_dir)\n",
        "    gen_acc, gen_cohen_kappa, per_class_r = test(fold_dir, fold_log_dir)\n",
        "\n",
        "    all_acc.append(gen_acc)\n",
        "    all_cohen_kappa.append(gen_cohen_kappa)\n",
        "    all_per_class_r = pd.concat((all_per_class_r, per_class_r), ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ceb9655",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 'class', 'acc', 'cohen_kappa'\n",
        "all_per_class_r = all_per_class_r.groupby(by='class').agg(\n",
        "    mean_acc=('acc', 'mean'),\n",
        "    std_acc=('acc','std'),\n",
        "    mean_kappa=('cohen_kappa', 'mean'),\n",
        "    std_kappa=('cohen_kappa','std')\n",
        ").reset_index(drop=True)\n",
        "\n",
        "all_per_class_r.to_csv(osp.join(log_dir, 'per_class_result.csv'), index=False)\n",
        "\n",
        "result = f\"\"\"Mean acc: f{np.mean(all_acc):.4f}. Std acc: f{np.std(all_acc):.4f}\n",
        "Mean kappa: f{np.mean(all_cohen_kappa):.4f}. Std kappa: f{np.std(all_cohen_kappa):.4f}\n",
        "\"\"\"\n",
        "with open(osp.join(log_dir, 'results.txt'), 'w') as f:\n",
        "    f.write(result)\n",
        "print(result)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "15134e55",
      "metadata": {},
      "source": [
        "### Visualization\n",
        "\n",
        "#### Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b225c34d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "#get the outputs, and labels\n",
        "def get_outputs(model, dataload, device):\n",
        "    outputs_by_layer = None\n",
        "    all_labels = None\n",
        "\n",
        "    #get a batch from the dataload\n",
        "    for inputs, labels in dataload:\n",
        "        #move inputs to the correct device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.clone().detach().cpu().numpy()\n",
        "\n",
        "        #get the activations for visualization\n",
        "        outputs = model.get_output_per_layer(inputs)\n",
        "            \n",
        "        #save the outputs\n",
        "        if outputs_by_layer is None:\n",
        "            outputs_by_layer = outputs\n",
        "            all_labels       = labels\n",
        "        else:\n",
        "            for layer in outputs:\n",
        "                outputs_by_layer[layer] = np.concatenate((outputs_by_layer[layer], outputs[layer]), axis=0)\n",
        "            all_labels = np.concatenate((all_labels, labels))   \n",
        "\n",
        "    return outputs_by_layer, all_labels\n",
        "\n",
        "#maps from high dimension to 2D\n",
        "def projection(outputs_by_layer, reducer):\n",
        "    projection_by_layer = OrderedDict()\n",
        "\n",
        "    for layer in outputs_by_layer:\n",
        "        #get the output of layer\n",
        "        output = outputs_by_layer[layer]\n",
        "        output = output.reshape(output.shape[0], -1)\n",
        "        #map to 2D\n",
        "        embedded = reducer.fit_transform(output)\n",
        "        #save projection\n",
        "        projection_by_layer[layer] = embedded\n",
        "  \n",
        "    return projection_by_layer\n",
        "\n",
        "#plot the projection of the output of each layer\n",
        "def create_visualization(projection_by_layer, all_labels, out_dir):\n",
        "  \n",
        "    for layer in projection_by_layer:\n",
        "        embedded = projection_by_layer[layer]\n",
        "  \n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(embedded[:, 0], embedded[:, 1], c=all_labels, cmap=plt.get_cmap('tab10'))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(layer)\n",
        "        plt.colorbar()\n",
        "        plt.savefig(osp.join(out_dir, layer)+'.pdf')\n",
        "        plt.show()\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e2fb06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#reducer = umap.UMAP()\n",
        "reducer = TSNE(perplexity=30)\n",
        "\n",
        "\n",
        "outputs_by_layer, all_labels = get_outputs(model, test_loader, device)\n",
        "projection_by_layer = projection(outputs_by_layer, reducer)\n",
        "\n",
        "create_visualization(projection_by_layer, all_labels, osp.join(log_dir, 'tsne'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ad59ae25",
      "metadata": {},
      "source": [
        "#### GRAD_CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5baab8e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_activations(model, x, device):\n",
        "    # move input tensor x to the selected device\n",
        "    x = x.to(device)\n",
        "    # get activations after feature extraction\n",
        "    x1 = model.conv1(x)\n",
        "    x2 = model.conv2(x1)\n",
        "    x1 = torch.nn.functional.interpolate(x1, scale_factor=(0.5, 0.5), mode='nearest',\n",
        "                                         recompute_scale_factor=True) \n",
        "    x  = torch.cat((x1, x2), dim=1) \n",
        "\n",
        "    return x\n",
        "\n",
        "def get_output_of_the_model(model, x, device):\n",
        "    # put the model in the evaluation mode\n",
        "    model.eval() \n",
        "    # move input tensor x to the selected device\n",
        "    x = x.to(device) \n",
        "    # execute the model with gradients\n",
        "    output = model(x)\n",
        "    return(output)\n",
        "\n",
        "def get_heatmap(model, x, device, labels_map):\n",
        "    # add one dimension (dim=0 must be the number of images)\n",
        "    xin = x.unsqueeze(0)\n",
        "    xin.to\n",
        "    # get the output of the feature extractor \n",
        "    activ  = model.get_activations(model, xin, device)\n",
        "        \n",
        "    # get the predictions at the output of the decision layer\n",
        "    logits = get_output_of_the_model(model, xin, device)\n",
        "    \n",
        "    # get the most confident prediction\n",
        "    pred   = logits.max(-1)[-1] \n",
        "    print(\"predicted label: \", labels_map[pred.cpu().detach().numpy()[0]])\n",
        "    \n",
        "    # compute gradients with respect to the most confident prediction\n",
        "    model.zero_grad() \n",
        "    logits[0,pred].backward(retain_graph=True) \n",
        "    \n",
        "    \n",
        "    print(activ.shape)\n",
        "    # only conv2D generates gradients in the feature extractor: extracts them.  \n",
        "    for layer_name, layer in model.conv1.named_children():\n",
        "        if layer_name == '0':\n",
        "            grad1 = layer.weight.grad\n",
        "            nchannels1 = grad1.shape[0]\n",
        "            # compute the weighted mean of the activations across channels using the mean value of the gradient \n",
        "            # of each filter parameter as weight. \n",
        "            for i in range(16):\n",
        "                activ[:,i,:,:] *= grad1[i].mean()\n",
        "    for layer_name, layer in model.conv2.named_children():\n",
        "        if layer_name == '0':\n",
        "            grad2 = layer.weight.grad\n",
        "            nchannels2 = grad2.shape[0]     \n",
        "            for i in range(64):\n",
        "                activ[:,i+16,:,:] *= grad2[i].mean()\n",
        "                \n",
        "    heatmap = torch.mean(activ, dim=1)[0].cpu().detach()\n",
        "\n",
        "    # convert to numpy, normalize, and resize to the input size\n",
        "    heatmap = heatmap.squeeze(0).numpy()\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "    heatmap = cv2.resize(heatmap, (x.shape[2], x.shape[1]))\n",
        "    return(heatmap)\n",
        "\n",
        "def display_image_with_heatmap(img, heatmap, scale):\n",
        "    heatmap = np.uint8(255.0*heatmap)\n",
        "    width   = int(heatmap.shape[1]*scale)\n",
        "    height  = int(heatmap.shape[0]*scale)\n",
        "    heatmap = cv2.resize(heatmap, (width, height))\n",
        "    img     = cv2.resize(255*img, (width, height))\n",
        "    heatmap = cv2.applyColorMap(255-heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = np.uint8(heatmap)\n",
        "    heatmap = np.uint8(heatmap*0.7 + img*0.3)\n",
        "    plt.imshow(heatmap)\n",
        "    plt.show()\n",
        "\n",
        "# get the first image tensor\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "\n",
        "x          = test_set[20][0]\n",
        "true_label = test_set[20][1]\n",
        "print(\"true label: \", labels_map[true_label])\n",
        "heatmap = get_heatmap(model, x, device, model_name, labels_map)\n",
        "x       = x.permute(1,2,0).numpy()\n",
        "if (x.shape[2]==1): # convert it to three channels\n",
        "    x = x.squeeze(2)\n",
        "    x = np.stack((x,)*3,axis=-1)  \n",
        "display_image_with_heatmap(x, heatmap, 4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
